# Optimizaci√≥n de Modelos de Clasificaci√≥n para Dispositivos M√≥viles con Recursos Limitados

> Trabajo Fin de M√°ster (VIU, 2025). Investigaci√≥n aplicada para equilibrar **precisi√≥n y eficiencia** en clasificaci√≥n de im√°genes ejecutada **on-device** en m√≥viles y sistemas embebidos.



## üìå Descripci√≥n
Este proyecto eval√∫a t√©cnicas de optimizaci√≥n para llevar modelos de **aprendizaje profundo** a dispositivos con **memoria, c√≥mputo y energ√≠a limitados**. Se compara un **Teacher** de alta capacidad (**Vision Transformer ‚Äì ViT Base**) frente a un **Student** eficiente (**MobileNetV3 Small**), entrenado en modo cl√°sico y con **Knowledge Distillation (KD)**.  
El estudio se realiza sobre **Food-101** (101 clases, >100k im√°genes), un escenario multiclase exigente y representativo para despliegue m√≥vil



## üéØ Objetivos
- Investigar la viabilidad de clasificaci√≥n multiclase **on-device** manteniendo precisi√≥n competitiva.
- Entrenar un **Teacher (ViT Base)** como referencia y un **Student (MobileNetV3 Small)** en dos variantes: **cl√°sico** y **con KD**.
- Comparar **precisi√≥n**, **tama√±o del modelo**, **latencia** (proyecci√≥n a m√≥vil) y **uso de memoria**.
- Demostrar que **KD** mejora la **generalizaci√≥n** del Student manteniendo un tama√±o reducido (‚â§10 MB; ideal ‚â§5 MB).



## ‚öôÔ∏è Metodolog√≠a
- **Dataset**: [Food-101](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/)  
- **Modelos**:
  - **Teacher**: ViT Base (gran capacidad, gu√≠a del conocimiento).
  - **Student**: MobileNetV3 Small (eficiente para m√≥viles).
- **Entrenamientos**:
  1) Student **cl√°sico**.  
  2) Student **con KD** (logits del Teacher con temperatura T y mezcla Œ±).
- **Evaluaci√≥n**:
  - M√©tricas de clasificaci√≥n (Accuracy, F1 macro), **matriz de confusi√≥n** y an√°lisis por clase.
  - **Tama√±o del modelo** (MB) y **latencia** (medici√≥n en PC + proyecci√≥n a m√≥vil por gama).
  - Discusi√≥n de compromisos **precisi√≥n ‚Üî eficiencia** para despliegue real.



## üìä Resultados principales

| Modelo                         | Rol      | Tama√±o aprox. | Accuracy test | Macro F1 | Hallazgos clave |
|--------------------------------|----------|---------------|---------------|----------|-----------------|
| **ViT Base**                   | Teacher  | ~327 MB       | **0.97**      | **0.97** | Modelo de referencia con alt√≠sima precisi√≥n. Sin embargo, su gran tama√±o y coste computacional lo hacen inviable para m√≥viles. |
| **MobileNetV3 Small**          | Student  | ~6.3 MB       | **0.73**      | **0.73** | Modelo ligero y eficiente, pero con claras limitaciones de generalizaci√≥n en un escenario multiclase complejo como Food-101. |
| **MobileNetV3 Small + KD**     | Student  | ~6.3 MB       | **0.90**      | **0.89** | La Knowledge Distillation mejora sustancialmente el rendimiento del Student, alcanzando un nivel cercano al Teacher con tama√±o reducido y latencias bajas. |

> Resumen: **KD logra un salto del 73% al 90% en accuracy**, validando su eficacia para optimizar modelos ligeros sin incrementar el tama√±o, lo que lo hace adecuado para **dispositivos m√≥viles de gama media y baja**.



## üß™ Reproducibilidad

### 1) Requisitos
- Python 3.10+
- PyTorch + torchvision
- timm, scikit-learn, numpy, matplotlib, tqdm, yaml

Instalaci√≥n r√°pida:
```bash
pip install -r requirements.txt
